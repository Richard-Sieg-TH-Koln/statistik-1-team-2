{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/marc/.pyenv/versions/3.9.1/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from germansentiment import SentimentModel\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"../data/raw/songs_complete_final.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentimentModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mps_device = torch.device(\"mps\")\n",
    "model.model.to(mps_device)\n",
    "model.device = \"mps\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes, probabilities = model.predict_sentiment(dataset[\"lyrics\"][:10], output_probabilities = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('negative',\n",
      "  [['positive', 0.012294584885239601],\n",
      "   ['negative', 0.9867066740989685],\n",
      "   ['neutral', 0.000998738338239491]],\n",
      "  'Powpow  Dicka, das kein Rap mehr, das ist Kindergarten Ihr wollt euch '\n",
      "  'treffen, doch dann lass ich eure Mütter warten Es ist KUKU TEAM, bringe '\n",
      "  'euch gemischte Karten Ihr könnt nicht rappen, dafür könnt ihr\\u205fso\\u205f'\n",
      "  'wie\\u205fBitches blasen Ich geb\\u205fdem Bastard jetzt\\u205feinn '\n",
      "  'Gnadenschuss Ihr seid keine Straßenjungs, ihr färbt euch die Haare bunt '\n",
      "  'Hör, mein Magen knurrt, Köfte im Fladenbrot Mach ein Nickerchen auf Sofa so '\n",
      "  'wie Baba Packs, Waage, Flex, Dicka, dis kein StanniKurs Was ist das für '\n",
      "  'Blick Moruk, als ob deine Mami furzt Heiße Aura wie Solarium Besser '\n",
      "  'verschwinde aus mein Radius Immer gute Lage so wie Mehringdamm Bares über '\n",
      "  'Telegram, Ware wie ein Chemiker Ihr Huren seid mir scheißegal Ob legal, '\n",
      "  'illegal, ist mir scheißegal'),\n",
      " ('negative',\n",
      "  [['positive', 0.012251140549778938],\n",
      "   ['negative', 0.9760990738868713],\n",
      "   ['neutral', 0.011649759486317635]],\n",
      "  'Gefährliche, gefährliche KiKiKiKi Gefährliche KiKiKiKi Gefährliche KiKiKiKi '\n",
      "  'Gefährliche KiKiKiKi Kobra Militär Ja, ja, ja, gib ihm  Du weißt doch, '\n",
      "  'Jubel kommt erst, wenn es klappt ja Sie warten drauf, dass ich verkack ja '\n",
      "  'Trubel im Kopf jeden Tag ja Es bedeutet nicht viel, wenn ich lach nein Es '\n",
      "  'bedeutet erst viel, wenn ich mach ja Erfolg entsteht nicht über Nacht nein '\n",
      "  'Jubel kommt erst, wenn es klappt ja Vorher Kugeln auf mich wie auf Pac  '\n",
      "  'Werf kein Auge auf mich, besser schließ sie Hol dir Feuer und zünde an, '\n",
      "  'easy Die schwarzen Wolken kommn nicht vom dem Weezy Mein Film ist viel zu '\n",
      "  'doll, ruf nach Scorsese Oder Tarantino, Vater war Rapstar mit MelodieFlows '\n",
      "  'Wie Kennedy tot, Memory droht, cest la vie Not Sie war erst sechs und ihr '\n",
      "  'Enemy rot Diablo Das Leben will mich zum Idiot zum Idiot Insta sagt, alles '\n",
      "  'im Lot jaja Die Hater sagen, sie fliegt hoch nein, nein, nein Aber niemals, '\n",
      "  'ich kämpfe hier nur für mein Brot Suche den richtigen Ton Nur für einn '\n",
      "  'mickrigen, mickrigen Lohn no Verticke Geschichten im Rohen oh no Lasse die '\n",
      "  'Augen doch ruhen Ich lasse nicht ab von dem Thron'),\n",
      " ('neutral',\n",
      "  [['positive', 0.0054069021716713905],\n",
      "   ['negative', 0.020907755941152573],\n",
      "   ['neutral', 0.9736853837966919]],\n",
      "  'Aus meiner Stadt fliegen Leuchtclips und Breslauer Kann sein, dass du '\n",
      "  'enttäuscht bist, wenn ich dir ein deutliches Brett haue Zwanzig Jahre '\n",
      "  'Terror aus der Mainstadt UF, zwanzig Jahre sterben für die Eintracht '\n",
      "  'Zwanzig Jahre Moseleck, zwanzig Jahre Drogengeld Frag deinn Vadder ma, seit '\n",
      "  'zwanzig Jahren wird hier so gerappt Denn ich bin Grönemeyer für die '\n",
      "  'Streetchabos Ich hab schöne Eier und ich lieb Chaos Sag, wie heißt die '\n",
      "  'Stadt, wo du verlierst, Alder Die Stadt, in der wir alles dominiern, Alder '\n",
      "  'Die Stadt, in der die Banken kontrolliern, Alder Hier blickst du bei Action '\n",
      "  'in die Waffe und du stirbst, Alder Zwanzig Jahre Trommeln auf dem Asphalt '\n",
      "  'Zwanzig Jahre jagen wir die Schmocks schon durch den Stadtwald Nenn mich '\n",
      "  'Nike, wenn du angeknockt schwächelst Verpasst man dir einn Gong, dass deine '\n",
      "  'Wangenknochen brechen  Seit zwanzig Jahrn gibt es heut den Adler mit dem '\n",
      "  'Kranz am Hals Und stecken immer noch dem Rest der Erde unsern Schwanz inn '\n",
      "  'Spalt In dieser Stadt, wo die Gewalt zum guten Ton gehört Und man die Diva '\n",
      "  'liebt, von Hessens Norden bis zum OpelWerk Ich hab gelitten und gekämpft '\n",
      "  'für diese Gruppe Mein Herz und meine Leidenschaft geschenkt für diese '\n",
      "  'Gruppe Fünf Jahre Stadionverbot erlebt in dieser Gruppe Mit paar Brüdern, '\n",
      "  'die dich killn, also rede nicht, du Fotze Ich bin ein Kurvenkind, die halbe '\n",
      "  'Welt bereist mit euch Meist abgefuckt und eingezäunt, doch niemals diese '\n",
      "  'Zeit bereut Ich bleibe Ultra und kein BallermannIdiot Und meine Crew bleibt '\n",
      "  'Nummer eins von Alemania bis zum Mond'),\n",
      " ('negative',\n",
      "  [['positive', 0.013118123635649681],\n",
      "   ['negative', 0.9868316650390625],\n",
      "   ['neutral', 5.024375786888413e-05]],\n",
      "  'Because youre so sweet You lift up my heart And Ill fall back again But I '\n",
      "  'dont know Quite\\u2005which\\u2005way to turn  The\\u2005light in your eyes '\n",
      "  'Touches the sky And\\u2005Im your man again And you hold me As close as '\n",
      "  'close\\u205fcan\\u205fbe  And\\u205fmanys the time That\\u205fyouve enchanted '\n",
      "  'me I\\u205fwant you back Here in my loving arms For eternity   You look and '\n",
      "  'you smile Warm me inside Cause youre so good to me A beauty Filled with the '\n",
      "  'joys of spring'),\n",
      " ('neutral',\n",
      "  [['positive', 0.02248389460146427],\n",
      "   ['negative', 0.028418289497494698],\n",
      "   ['neutral', 0.9490978717803955]],\n",
      "  '. Liquid Swords  GZA  Actual . souljaboytellem.com  Soulja Boy  . Only '\n",
      "  'Built  Cuban Linx  Raekwon  . Enter the WuTang  Chambers  WuTang Clan  . '\n",
      "  'Death Certificate  Ice Cube  . ATLiens  OutKast  . The Infamous  Mobb Deep  '\n",
      "  '. Doggystyle  Snoop Dogg  . Illmatic  Nas  . Ready to Die  Biggie Smalls  . '\n",
      "  'Stress The Extinction Agenda  Organized Konfusion  . Aquemini  OutKast  . '\n",
      "  'The Low End Theory  A Tribe Called Quest  . Whut Thee Album  Redman  . '\n",
      "  'Live  Let Die  Kool G Rap  DJ Polo  . It Takes a Nation of Millions to Hold '\n",
      "  'Us Back  Public Enemy  . Madvillainy  Madvillain  . AmeriKKKas Most Wanted  '\n",
      "  'Ice Cube  . Ironman  Ghostface Killah  . The Sun Rises in the East  Jeru '\n",
      "  'the Damaja  .  Midnight Marauders  A Tribe Called Quest  . ,,  Kool G Rap  '\n",
      "  '. Resurrection  Common  . The Chronic  Dr. Dre  . The Diary  Scarface  . '\n",
      "  'Reasonable Doubt  JayZ  . Bizarre Ride II The Pharcyde  The Pharcyde  . '\n",
      "  'Capital Punishment  Big Pun  . The Eminem Show  Eminem  . Fear of a Black '\n",
      "  'Planet  Public Enemy  . Supreme Clientele  Ghostface Killah  . Mos Def  '\n",
      "  'Talib Kweli are Black Star  Black Star  . Lifestylez ov da Poor  Dangerous  '\n",
      "  'Big L  . Paid in Full  Eric B.  Rakim  . Me Against the World  Pac  . To '\n",
      "  'Pimp a Butterfly  Kendrick Lamar  . Dare iz a Darkside  Redman  . '\n",
      "  'Southernplayalisticadillacmuzik  OutKast  . The College Dropout  Kanye '\n",
      "  'West  . Illadelph Halflife  The Roots  . Like Water for Chocolate  Common  '\n",
      "  '. My Beautiful Dark Twisted Fantasy  Kanye West  . Hell on Earth  Mobb '\n",
      "  'Deep  . Its Dark and Hell is Hot  DMX  . The Black Album  JayZ  . The '\n",
      "  'Marshall Mathers LP  Eminem  . WordLife   O.C.  . Moment of Truth  Gang '\n",
      "  'Starr  . Breaking Atoms  Main Source  . Goodfellas  Showbiz  A.G.  . Enta '\n",
      "  'Da Stage  Black Moon'),\n",
      " ('negative',\n",
      "  [['positive', 0.04864656180143356],\n",
      "   ['negative', 0.9505897164344788],\n",
      "   ['neutral', 0.0007636944646947086]],\n",
      "  'Yo, its that boy Denzel Curry Im finna do this for my niggas, mane Its the '\n",
      "  'intro, nigga I hope yall enjoy, it nigga, ya know what Im saying This the '\n",
      "  'King Remembered Underground Tape  to motherfuckin , my nigga Raider Klan is '\n",
      "  'in this bitch, shoutout to SpaceGhostPurrp, my nigga Yall better like... '\n",
      "  'yall better support a nigga, bruh Dat, yeee'),\n",
      " ('negative',\n",
      "  [['positive', 0.026630887761712074],\n",
      "   ['negative', 0.5013120174407959],\n",
      "   ['neutral', 0.47205713391304016]],\n",
      "  'Kippe Mischen und verliere meine Daten Steuerung und S das was gut ist hab '\n",
      "  'ich safe Kaufe Gin und ein paar Mischen formatieren meine Datenträger Poppe '\n",
      "  'Bottles und ich fühl mich wie ein Kind Keine Fehler weil es neu ist mache '\n",
      "  'Fehler weil ich will Wenn ich will krieg ich alles was ich will Aber fühl '\n",
      "  'mich dabei leer schalte ab und leg mich hin Tausеnd Menschen und ich fühle '\n",
      "  'mich allеine ja Ich will mit dem System interagieren finde nur keine '\n",
      "  'Treiber ja Heute geh ich schwimmen in der Sonne morgen sinke ich im tiefen '\n",
      "  'Wasser Ich bin heut ein guter Junge aber morgen dann ein ziemlich krasser '\n",
      "  'Motherfucker Heute bin ich da für die Welt morgen halte ich von allem '\n",
      "  'Abstand Weiß nie wann es reicht ich bin müde doch entscheide mich für noch '\n",
      "  'nen Upper Ich bin völlig unter Strom hab nen Lauf Dann kontakt mit nem '\n",
      "  'Jibbet RCDs lösen aus Sie will ins Bett auf Acid geh zwar darauf ein doch '\n",
      "  'bin nicht down Sehe Fabelwesen zieh mich wieder an und renne raus ja Ich '\n",
      "  'verwerfe meine Pläne leb im Traum Und mein Papierkorb leert sich pünktlich '\n",
      "  'alle  Tage aus Copy paste jeden Tag Emotionen sind nicht da Und ich fahre '\n",
      "  'viel zu schnell durch jede Kurve ist mir völlig egal Heute geh ich '\n",
      "  'schwimmen in der Sonne morgen sinke ich im tiefen Wasser Ich bin heut ein '\n",
      "  'guter Junge aber morgen dann ein ziemlich krasser Motherfucker Heute bin '\n",
      "  'ich da für die Welt morgen halte ich von allem Abstand Weiß nie wann es '\n",
      "  'reicht ich bin müde doch entscheide mich für noch nen Upper'),\n",
      " ('negative',\n",
      "  [['positive', 0.02080969139933586],\n",
      "   ['negative', 0.9790260195732117],\n",
      "   ['neutral', 0.00016435877478215843]],\n",
      "  'Yo pls dont call me Im busy at the moment U may call the receptionist He '\n",
      "  'will pass\\u2005the\\u2005call to me Chorus Started\\u2005from the bottom now '\n",
      "  'we here All\\u2005my drip is invested in my head Lost my life now\\u205f'\n",
      "  'Im\\u205ftryna\\u205fget it back Who\\u205fare u u\\u205flook like u drive a '\n",
      "  'van Pretty deep cause you know Im kinda black  we were all oppressed '\n",
      "  'Reminiscing alot of things about the past Money heist Im the man behind the '\n",
      "  'mask Verse Its a long journey of my life Long trips tryna bite now I show I '\n",
      "  'get paid No please dont pretend Im proud cause Im self made I survive when '\n",
      "  'they set traps Long shots on my lane No please dont pretend Repeat when '\n",
      "  'they play mine Press skip when they play urs Drip drip nangutatakho No trip '\n",
      "  'Nangu Yanga U gon get shot When u trespass Do u get that Do u dig that Do u '\n",
      "  'know that on my part'),\n",
      " ('negative',\n",
      "  [['positive', 0.3960035443305969],\n",
      "   ['negative', 0.5831013321876526],\n",
      "   ['neutral', 0.02089507505297661]],\n",
      "  'yýlýnda Refahyol hükümetinin iki liderine taþlama olarak yazýlmýþtýr... Bu '\n",
      "  'yoksulluk beni delledi Aldý aklým yaktý külledi Bunu yapan iki kiþi Biri '\n",
      "  'erkek biri diþi Halden bilmez iki kiþi Vay.. Çilli kedi, çilli kedi vay '\n",
      "  'Ciðerimi yedin kedi vay Bunu yapan iki kiþi Biri erkek biri diþi Halden '\n",
      "  'bilmez iki kiþi Vay bee..'),\n",
      " ('neutral',\n",
      "  [['positive', 0.037880685180425644],\n",
      "   ['negative', 0.04872855544090271],\n",
      "   ['neutral', 0.9133907556533813]],\n",
      "  'Studio AlbumsThe Rolling Stones , UK  Englands Newest Hit Makers , US  X  , '\n",
      "  'US The Rolling Stones No.  , UK  The Rolling Stones, Now , US Out of Our '\n",
      "  'Heads  Decembers Children And Everybodys , US Aftermath Between the '\n",
      "  'Buttons  Their Satanic Majesties Request  Beggars Banquet  Let It Bleed  '\n",
      "  'Sticky Fingers  Exile on Main St  Goats Head Soup  Its Only Rock n Roll  '\n",
      "  'Black and Blue  Some Girls  Emotional Rescue  Tattoo You  Undercover  Dirty '\n",
      "  'Work  Steel Wheels  Voodoo Lounge  Bridges to Babylon  A Bigger Bang  Blue  '\n",
      "  'Lonesome Live albums \\tGot Live If You Want It  \\tGet Yer YaYas Out The '\n",
      "  'Rolling Stones in Concert  \\tLove You Live  \\tStill Life  \\tFlashpoint  \\t'\n",
      "  'Stripped  \\tThe Rolling Stones Rock and Roll Circus  \\tNo Security  \\tLive '\n",
      "  'Licks  \\tShine a Light  \\tBrussels Affair Live   Some Girls Live in '\n",
      "  'Texas   \\tHampton Coliseum Live   L.A. Friday Live   Live at the '\n",
      "  'Checkerboard Lounge, Chicago   Live at the Tokyo Dome Live   Light the Fuse '\n",
      "  'Live   Live at Leeds Live   Sweet Summer Sun  Marquee Club Live   Sticky '\n",
      "  'Fingers Live  Havana MoonCompilation albums \\tBig Hits High Tide and Green '\n",
      "  'Grass  \\tFlowers  \\tThrough the Past, Darkly Big Hits Vol.   \\tStone Age  '\n",
      "  'Gimme Shelter  Hot Rocks   \\tMilestones  Rock n Rolling Stones  More Hot '\n",
      "  'Rocks Big Hits  Fazed Cookies  \\tNo Stone Unturned  \\tMetamorphosis   Made '\n",
      "  'in the Shade  Rolled Gold The Very Best of the Rolling Stones  \\tGet '\n",
      "  'Stoned  Greatest Hits  \\tTime Waits for No One  \\tSolid Rock  \\tSlow '\n",
      "  'Rollers  \\tSucking in the Seventies  \\tStory of The Stones  \\tRewind   \\t'\n",
      "  'Singles Collection The London Years  \\tJump Back The Best of The Rolling '\n",
      "  'Stones  \\tForty Licks  \\tRarities   \\tGRRR  \\tNo Stone Unturned Vol.   Box '\n",
      "  'setsOriginal Master Recordings  The Rolling Stones   The Rolling Stones   '\n",
      "  'You Get What You Need  The Rolling Stones In Mono Special reissuesSome '\n",
      "  'Girls  Exile On Main St.')]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "pprint(list(zip(classes, probabilities, dataset[\"lyrics\"][:10])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine Tune Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0, 0, 0, ..., 1, 1, 1]), Index(['Rock', 'Rap', 'Pop', 'Schlager'], dtype='object'))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "ds = pd.read_csv(\"/Users/marc/Desktop/statistik-1-team-2/data/raw/songs_complete_final.csv\")\n",
    "ds.dropna(subset=[\"genre_cat\"], inplace=True)\n",
    "ds[\"genre_cat\"].unique()\n",
    "factorized = pd.factorize(ds[\"genre_cat\"])\n",
    "print(factorized)\n",
    "ds[\"labels\"] = factorized[0]\n",
    "ds.to_csv(\"/Users/marc/Desktop/statistik-1-team-2/data/raw/songs_complete_final_no_nan.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Downloading and preparing dataset csv/default to /Users/marc/.cache/huggingface/datasets/csv/default-06ad6ddce1ff2f54/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files: 100%|██████████| 1/1 [00:00<00:00, 3880.02it/s]\n",
      "Extracting data files: 100%|██████████| 1/1 [00:00<00:00, 244.98it/s]\n",
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to /Users/marc/.cache/huggingface/datasets/csv/default-06ad6ddce1ff2f54/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 430.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['artist', 'artist_id', 'album', 'album_id', 'release_date', 'title', 'full_title', 'song_id', 'lyrics', 'release_year', 'weekday', 'genre', 'genre_cat', 'word_count', 'labels'],\n",
      "        num_rows: 16429\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "hf_dataset = load_dataset(\"csv\", data_files=\"/Users/marc/Desktop/statistik-1-team-2/data/raw/songs_complete_final_no_nan.csv\")\n",
    "# hf_dataset=hf_dataset.rename_columns({\"genre_cat\": \"labels\"})\n",
    "print(hf_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = hf_dataset[\"train\"].train_test_split(test_size=0.2)\n",
    "train_ds = split[\"train\"]\n",
    "test_ds = split[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)okenizer_config.json: 100%|██████████| 29.0/29.0 [00:00<00:00, 11.7kB/s]\n",
      "Downloading (…)lve/main/config.json: 100%|██████████| 433/433 [00:00<00:00, 411kB/s]\n",
      "Downloading (…)solve/main/vocab.txt: 100%|██████████| 255k/255k [00:00<00:00, 923kB/s]\n",
      "Downloading (…)/main/tokenizer.json: 100%|██████████| 485k/485k [00:00<00:00, 4.93MB/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-german-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"lyrics\"], padding=\"max_length\", truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    }
   ],
   "source": [
    "train_tokenized = train_ds.map(tokenize_function, batched=True)\n",
    "test_tokenized = test_ds.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_train_dataset = train_tokenized.shuffle(seed=42).select(range(1000))\n",
    "small_test_dataset = test_tokenized.shuffle(seed=42).select(range(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'Rock', 1: 'Rap', 2: 'Pop', 3: 'Schlager'}\n"
     ]
    }
   ],
   "source": [
    "id2label = {idx: val for idx, val in enumerate(factorized[1])}\n",
    "print(id2label)\n",
    "label2id = {val: key for (key, val) in id2label.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading model.safetensors: 100%|██████████| 439M/439M [00:07<00:00, 61.0MB/s] \n",
      "Some weights of the model checkpoint at bert-base-german-cased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-german-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-german-cased\", num_labels=4, id2label=id2label, label2id=label2id)\n",
    "# model.to(\"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\n",
      "Copy-and-paste the text below in your GitHub issue\n",
      "\n",
      "- `Accelerate` version: 0.20.1\n",
      "- Platform: macOS-13.4.1-arm64-arm-64bit\n",
      "- Python version: 3.9.1\n",
      "- Numpy version: 1.25.0\n",
      "- PyTorch version (GPU?): 2.0.1 (False)\n",
      "- PyTorch XPU available: False\n",
      "- System RAM: 16.00 GB\n",
      "- `Accelerate` default config:\n",
      "\t- compute_environment: LOCAL_MACHINE\n",
      "\t- distributed_type: NO\n",
      "\t- mixed_precision: fp16\n",
      "\t- use_cpu: True\n",
      "\t- num_processes: 1\n",
      "\t- machine_rank: 0\n",
      "\t- num_machines: 1\n",
      "\t- rdzv_backend: static\n",
      "\t- same_network: False\n",
      "\t- main_training_function: main\n",
      "\t- downcast_bf16: False\n",
      "\t- tpu_use_cluster: False\n",
      "\t- tpu_use_sudo: False\n"
     ]
    }
   ],
   "source": [
    "!accelerate env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Using the `Trainer` with `PyTorch` requires `accelerate>=0.20.1`: Please run `pip install transformers[torch]` or `pip install accelerate -U`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m training_args \u001b[39m=\u001b[39m TrainingArguments(\n\u001b[1;32m      2\u001b[0m     output_dir\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mbert_genre\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      3\u001b[0m     use_mps_device\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m      4\u001b[0m     num_train_epochs\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m,\n\u001b[1;32m      5\u001b[0m     \u001b[39m# learning_rate=2e-5,\u001b[39;49;00m\n\u001b[1;32m      6\u001b[0m     \u001b[39m# per_device_train_batch_size=16,\u001b[39;49;00m\n\u001b[1;32m      7\u001b[0m     \u001b[39m# per_device_eval_batch_size=16,\u001b[39;49;00m\n\u001b[1;32m      8\u001b[0m     \u001b[39m# weight_decay=0.01,\u001b[39;49;00m\n\u001b[1;32m      9\u001b[0m     evaluation_strategy\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mepoch\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     10\u001b[0m     save_strategy\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mepoch\u001b[39;49m\u001b[39m\"\u001b[39;49m,)\n",
      "File \u001b[0;32m<string>:111\u001b[0m, in \u001b[0;36m__init__\u001b[0;34m(self, output_dir, overwrite_output_dir, do_train, do_eval, do_predict, evaluation_strategy, prediction_loss_only, per_device_train_batch_size, per_device_eval_batch_size, per_gpu_train_batch_size, per_gpu_eval_batch_size, gradient_accumulation_steps, eval_accumulation_steps, eval_delay, learning_rate, weight_decay, adam_beta1, adam_beta2, adam_epsilon, max_grad_norm, num_train_epochs, max_steps, lr_scheduler_type, warmup_ratio, warmup_steps, log_level, log_level_replica, log_on_each_node, logging_dir, logging_strategy, logging_first_step, logging_steps, logging_nan_inf_filter, save_strategy, save_steps, save_total_limit, save_safetensors, save_on_each_node, no_cuda, use_mps_device, seed, data_seed, jit_mode_eval, use_ipex, bf16, fp16, fp16_opt_level, half_precision_backend, bf16_full_eval, fp16_full_eval, tf32, local_rank, ddp_backend, tpu_num_cores, tpu_metrics_debug, debug, dataloader_drop_last, eval_steps, dataloader_num_workers, past_index, run_name, disable_tqdm, remove_unused_columns, label_names, load_best_model_at_end, metric_for_best_model, greater_is_better, ignore_data_skip, sharded_ddp, fsdp, fsdp_min_num_params, fsdp_config, fsdp_transformer_layer_cls_to_wrap, deepspeed, label_smoothing_factor, optim, optim_args, adafactor, group_by_length, length_column_name, report_to, ddp_find_unused_parameters, ddp_bucket_cap_mb, dataloader_pin_memory, skip_memory_metrics, use_legacy_prediction_loop, push_to_hub, resume_from_checkpoint, hub_model_id, hub_strategy, hub_token, hub_private_repo, gradient_checkpointing, include_inputs_for_metrics, fp16_backend, push_to_hub_model_id, push_to_hub_organization, push_to_hub_token, mp_parameters, auto_find_batch_size, full_determinism, torchdynamo, ray_scope, ddp_timeout, torch_compile, torch_compile_backend, torch_compile_mode, xpu_backend)\u001b[0m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.1/lib/python3.9/site-packages/transformers/training_args.py:1340\u001b[0m, in \u001b[0;36mTrainingArguments.__post_init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1334\u001b[0m     \u001b[39mif\u001b[39;00m version\u001b[39m.\u001b[39mparse(version\u001b[39m.\u001b[39mparse(torch\u001b[39m.\u001b[39m__version__)\u001b[39m.\u001b[39mbase_version) \u001b[39m==\u001b[39m version\u001b[39m.\u001b[39mparse(\u001b[39m\"\u001b[39m\u001b[39m2.0.0\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfp16:\n\u001b[1;32m   1335\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m--optim adamw_torch_fused with --fp16 requires PyTorch>2.0\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1337\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   1338\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mframework \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1339\u001b[0m     \u001b[39mand\u001b[39;00m is_torch_available()\n\u001b[0;32m-> 1340\u001b[0m     \u001b[39mand\u001b[39;00m (\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdevice\u001b[39m.\u001b[39mtype \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1341\u001b[0m     \u001b[39mand\u001b[39;00m (get_xla_device_type(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice) \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mGPU\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1342\u001b[0m     \u001b[39mand\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfp16 \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfp16_full_eval)\n\u001b[1;32m   1343\u001b[0m ):\n\u001b[1;32m   1344\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1345\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFP16 Mixed precision training with AMP or APEX (`--fp16`) and FP16 half precision evaluation\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1346\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m (`--fp16_full_eval`) can only be used on CUDA devices.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1347\u001b[0m     )\n\u001b[1;32m   1349\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   1350\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mframework \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1351\u001b[0m     \u001b[39mand\u001b[39;00m is_torch_available()\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1356\u001b[0m     \u001b[39mand\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbf16 \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbf16_full_eval)\n\u001b[1;32m   1357\u001b[0m ):\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.1/lib/python3.9/site-packages/transformers/training_args.py:1764\u001b[0m, in \u001b[0;36mTrainingArguments.device\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1760\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1761\u001b[0m \u001b[39mThe device used by this process.\u001b[39;00m\n\u001b[1;32m   1762\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1763\u001b[0m requires_backends(\u001b[39mself\u001b[39m, [\u001b[39m\"\u001b[39m\u001b[39mtorch\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[0;32m-> 1764\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_setup_devices\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.1/lib/python3.9/site-packages/transformers/utils/generic.py:54\u001b[0m, in \u001b[0;36mcached_property.__get__\u001b[0;34m(self, obj, objtype)\u001b[0m\n\u001b[1;32m     52\u001b[0m cached \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(obj, attr, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m     53\u001b[0m \u001b[39mif\u001b[39;00m cached \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> 54\u001b[0m     cached \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfget(obj)\n\u001b[1;32m     55\u001b[0m     \u001b[39msetattr\u001b[39m(obj, attr, cached)\n\u001b[1;32m     56\u001b[0m \u001b[39mreturn\u001b[39;00m cached\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.1/lib/python3.9/site-packages/transformers/training_args.py:1672\u001b[0m, in \u001b[0;36mTrainingArguments._setup_devices\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1670\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_sagemaker_mp_enabled():\n\u001b[1;32m   1671\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_accelerate_available(min_version\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m0.20.1\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m-> 1672\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(\n\u001b[1;32m   1673\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mUsing the `Trainer` with `PyTorch` requires `accelerate>=0.20.1`: Please run `pip install transformers[torch]` or `pip install accelerate -U`\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1674\u001b[0m         )\n\u001b[1;32m   1675\u001b[0m     AcceleratorState\u001b[39m.\u001b[39m_reset_state(reset_partial_state\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m   1676\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdistributed_state \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: Using the `Trainer` with `PyTorch` requires `accelerate>=0.20.1`: Please run `pip install transformers[torch]` or `pip install accelerate -U`"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"bert_genre\",\n",
    "    use_mps_device=True,\n",
    "    num_train_epochs=2,\n",
    "    # learning_rate=2e-5,\n",
    "    # per_device_train_batch_size=16,\n",
    "    # per_device_eval_batch_size=16,\n",
    "    # weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "accuraracy = evaluate.load(\"accuracy\")\n",
    "recall = evaluate.load(\"recall\")\n",
    "precision = evaluate.load(\"precision\")\n",
    "f1 = evaluate.load(\"f1\")\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    results = {}\n",
    "    results.update(accuraracy.compute(predictions=predictions, references=labels))\n",
    "    results.update(recall.compute(predictions=predictions, references=labels, average=\"micro\"))\n",
    "    results.update(precision.compute(predictions=predictions, references=labels, average=\"micro\"))\n",
    "    results.update(f1.compute(predictions=predictions, references=labels, average=\"micro\"))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=small_train_dataset,\n",
    "    eval_dataset=small_test_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "pipe = pipeline(task=\"text-classification\", model=model, tokenizer=tokenizer, return_all_scores=True, device=\"mps\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toc1 = \"\"\"\n",
    "ch denke an das, was Du empfiehlst:\n",
    "\"Talent borrows, genius steals\"\n",
    "An das was Du mir anvertraust:\n",
    "\"Wähle dir deine fallen aus\"\n",
    "Ich glaub ich kanns erst jetzt verstehen\n",
    "Wir müssen durch den Spiegel gehen\n",
    "Danke auch für den Versuch\n",
    "Das hier ist kein Wörterbuch\n",
    "Du streichst mir über\n",
    "Mein Gesicht\n",
    "Gegen die Welt\n",
    "Gegen den Strich\n",
    "Meine Liebe\n",
    "Dein Verzicht\n",
    "Gegen die Welt\n",
    "Gegen den Strich\n",
    "Völker! Auf zum Gefecht!\n",
    "Die Illusion wird Menschenrecht\n",
    "Ich bin nicht allein in meiner Sucht\n",
    "Vor den Spießern auf der Flucht\n",
    "Du denkst an mich\n",
    "Ich denk' an Dich\n",
    "Gegen die Welt\n",
    "Gegen den Strich\n",
    "Unser Denken\n",
    "Verbündet sich\n",
    "Gegen die Welt\n",
    "Gegen den Strich\n",
    "Jeden Morgen, jede Nacht\n",
    "Jeden Morgen, jede Nacht\n",
    "Jeden Morgen, jede Nacht\n",
    "Jeden Morgen, jede Nacht\n",
    "Du streichst mir über\n",
    "Mein Gesicht\n",
    "Gegen die Welt\n",
    "Gegen den Strich\n",
    "Meine Liebe\n",
    "Dein Verzicht\n",
    "Gegen die Welt\n",
    "Gegen den Strich\n",
    "Du streichst mir über\n",
    "Mein Gesicht\n",
    "Gegen die Welt\n",
    "Gegen den Strich\n",
    "Meine Liebe\n",
    "Dein Verzicht\n",
    "Gegen die Welt\n",
    "Gegen den Strich\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kool1 = \"\"\"Ich komm auf die Bühne leg Hand ans Mic\n",
    "Fang langsam an und demolier dich am Ende wie Van Damme\n",
    "Geh hol deine Gun dann, wärs fair\n",
    "Doch bis dahin ist das einzge was du gewinnst nur Land man\n",
    "Guck euer Gerede macht Müde wie der Sandmann\n",
    "Ob Türke, Russe oder der Rest von Grand Cannes\n",
    "Bei jedem finden diese Tracks Anklang, aber was ist das\n",
    "Deine Bitch hat ein Loch wie'n Schuss von ner Pumpgun\n",
    "Na dann viel Fun man, ich an deiner Stelle würd\n",
    "Sie am randstehen lassen wie'n Wandschrank\n",
    "Egal kein tam-tam, wahrt den Anstand meine\n",
    "Texte ficken alles wie Matrosen auf Landgang\n",
    "Für mich ist rappen wie wandern, s läuft\n",
    "Und ich bediene gerne euer Team alleine die Bank lang\n",
    "Danke an alle die mein Talent verkannt ham\n",
    "Heute seht ihr wohl alles anders wie in nem Handstand, oder nicht?\n",
    "Nie, niemals kriegt ihr mich weg, nie wi-ieder geh' ich\n",
    "Keiner von euch kann's, wie di-ieser Typ ist da Vinci\n",
    "Und das hier ist meine Mona-Lisa\n",
    "Des Original, die Eins, vielmals von ihm kopiert\n",
    "Aber niemals erreicht, ihr meint ihr seids\n",
    "Doch kommt nicht ran an Mona-Lisa\n",
    "Ich renn' durch Beats wie'n Athlet\n",
    "Ich nehm' Wörter und Silben und kann sie im Takt dreh'n\n",
    "Des ist'n Fetisch, ähnlich wie bei Leuten, die auf Leder und Lack stehen\n",
    "Ich kann mich an meinen Texten nicht satt sehen\n",
    "Ich hab' den übelsten dich-glatt-bügelnden Flow\n",
    "Alles, was ich rappe, muss \"Zack Zack Zack\" gehen\n",
    "Sie wollen mein Album mehr, als Miss Milian nackt sehen\n",
    "Feature mit dir, ich muss ablehnen\n",
    "Was geht, alle nenn'n dich Kackface\n",
    "Als würde dein Kopf unmittelbar hinter dem Sack kleben\n",
    "Mich will jeder wie 'ne Wiese plattmähen\n",
    "Mich battlen lass', wie bei 'nem Liebesakt blähen\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe(kool1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit ('3.9.1')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fb408a8a50588f9d482ac690f7d1c5df410608bd4bf41691bb22e16d1eff6208"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
